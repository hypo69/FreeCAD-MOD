## ðŸ“„ `ai_client.py` 

```python
## \file AIAssistant/ai_client.py
# -*- coding: utf-8 -*-
#! .pyenv/bin/python3

"""
Universeller Client fÃ¼r Google Gemini, angepasst fÃ¼r FreeCAD.
UnterstÃ¼tzt das Senden von Text und Bildern mit Fehlerbehandlung und Wiederholungsversuchen.
"""

import asyncio
import time
from pathlib import Path
from typing import Optional, Dict, List, Any, Union
from io import IOBase
import google.generativeai as genai
from google.api_core.exceptions import (
    GatewayTimeout,
    RetryError,
    ServiceUnavailable,
    ResourceExhausted,
    InvalidArgument,
)
from google.auth.exceptions import (
    DefaultCredentialsError,
    RefreshError,
)
import FreeCAD

# === LOGGER FÃœR FREECAD ===
def log_info(msg: str) -> None:
    """Die Funktion schreibt eine Informationsmeldung in die FreeCAD-Konsole."""
    FreeCAD.Console.PrintMessage(f"[AIAssistant] {msg}\n")

def log_error(msg: str) -> None:
    """Die Funktion schreibt eine Fehlermeldung in die FreeCAD-Konsole."""
    FreeCAD.Console.PrintError(f"[AIAssistant] ERROR: {msg}\n")

def log_warning(msg: str) -> None:
    """Die Funktion schreibt eine Warnmeldung in die FreeCAD-Konsole."""
    FreeCAD.Console.PrintMessage(f"[AIAssistant] WARN: {msg}\n")

def log_debug(msg: str) -> None:
    """Die Funktion schreibt eine Debug-Meldung in die FreeCAD-Konsole."""
    FreeCAD.Console.PrintMessage(f"[AIAssistant] DEBUG: {msg}\n")

# --- Konstanten fÃ¼r Timeouts und Versuche ---
NETWORK_ERROR_MAX_ATTEMPTS = 5
SERVICE_UNAVAILABLE_MAX_ATTEMPTS = 3
INVALID_INPUT_MAX_ATTEMPTS = 3
INITIAL_RETRY_SLEEP_SECONDS = 2
NETWORK_RETRY_SLEEP_SECONDS = 120  # 2 Minuten
SERVICE_RETRY_SLEEP_SECONDS_BASE = 10
QUOTA_EXHAUSTED_SLEEP_SECONDS = 14400  # 4 Stunden

def normalize_answer(text: str) -> str:
    """Die Funktion bereinigt die Antwort von Markdown-Wrappern wie ```text ... ```."""
    text = text.strip()
    if text.startswith("```") and text.endswith("```"):
        lines = text.split("\n")
        if len(lines) >= 2:
            return "\n".join(lines[1:-1])
    if text.startswith("text\n"):
        text = text[5:]
    return text.strip()

def get_image_bytes(image_path: Path) -> Optional[bytes]:
    """Die Funktion liest das Bild als Bytes."""
    try:
        with open(image_path, "rb") as f:
            return f.read()
    except Exception as ex:
        log_error(f"Fehler beim Lesen der Bildbytes aus der Datei: {image_path}")
        return None

class GoogleGenerativeAi:
    """
    Klasse fÃ¼r die Interaktion mit Google Generative AI-Modellen.
    Angepasst fÃ¼r die Arbeit in FreeCAD ohne externe AbhÃ¤ngigkeiten.
    """

    def __init__(
        self,
        api_key: str,
        model_name: str = "gemini-2.5-flash",
        generation_config: Optional[Dict] = None,
        system_instruction: Optional[str] = None,
    ):
        """
        Initialisiert eine Instanz der Klasse GoogleGenerativeAi.

        Args:
            api_key (str): API-SchlÃ¼ssel fÃ¼r Google Generative AI.
            model_name (str): Name des Gemini-Modells, das verwendet werden soll.
            generation_config (Dict, optional): Konfiguration der Generierung.
            system_instruction (Optional[str], optional): Systemanweisung fÃ¼r das Modell.
        """
        self.api_key = api_key
        self.model_name = model_name
        self.generation_config = generation_config if generation_config is not None else {'response_mime_type': 'text/plain'}
        self.system_instruction = system_instruction

        try:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel(
                model_name=self.model_name,
                generation_config=self.generation_config,
                system_instruction=self.system_instruction,
            )
            log_info(f"Modell {self.model.model_name} initialisiert")
        except (DefaultCredentialsError, RefreshError) as ex:
            log_error("Fehler bei der Authentifizierung der Gemini API")
            raise
        except Exception as ex:
            log_error("Fehler beim Initialisieren des Gemini-Modells")
            raise

    def ask(self, q: str, attempts: int = 15, save_dialogue: bool = False,
            clean_response: bool = True, context: Optional[Union[str, List[str]]] = None) -> Optional[str]:
        """
        Die Methode sendet eine Textanfrage synchron an das Modell und gibt die Antwort zurÃ¼ck.

        Args:
            q (str): Textanfrage an das Modell.
            attempts (int): Anzahl der Versuche, die Anfrage zu senden.
            save_dialogue (bool): Flag zum Speichern des Dialogs (nicht in FreeCAD implementiert).
            clean_response (bool): Flag zum Bereinigen der Antwort von Code-Markup.
            context (Optional[Union[str, List[str]]]): ZusÃ¤tzlicher Kontext fÃ¼r das Modell.

        Returns:
            Optional[str]: Textantwort des Modells oder `None` im Fehlerfall.
        """
        content_to_send: List[Any] = []
        if context:
            context_str = "\n".join(context) if isinstance(context, list) else context
            content_to_send.append(f"Kontext:\n{context_str}\n")
            log_debug(f"RAG-Kontext zur Anfrage hinzugefÃ¼gt (LÃ¤nge: {len(context_str)} Zeichen).")
        content_to_send.append(q)

        for attempt in range(attempts):
            try:
                response = self.model.generate_content(content_to_send)
                if hasattr(response, 'text') and response.text:
                    response_text = response.text
                    return normalize_answer(response_text) if clean_response else response_text
                else:
                    sleep_time = INITIAL_RETRY_SLEEP_SECONDS ** attempt
                    log_debug(f"Keine Antwort vom Modell erhalten. Versuch: {attempt + 1}/{attempts}. Pause: {sleep_time} Sek.")
                    time.sleep(sleep_time)
                    continue
            except ResourceExhausted:
                log_error("Kontingentlimit Ã¼berschritten.")
                return "ResourceExhausted"
            except (DefaultCredentialsError, RefreshError):
                log_error("Authentifizierungsfehler.")
                return None
            except (GatewayTimeout, ServiceUnavailable):
                if attempt >= SERVICE_UNAVAILABLE_MAX_ATTEMPTS:
                    log_error(f"Dienst nach {SERVICE_UNAVAILABLE_MAX_ATTEMPTS} Versuchen nicht verfÃ¼gbar.")
                    break
                sleep_time_service = INITIAL_RETRY_SLEEP_SECONDS**attempt + SERVICE_RETRY_SLEEP_SECONDS_BASE
                log_error(f"Dienst nicht verfÃ¼gbar. Versuch: {attempt + 1}/{attempts}. Pause: {sleep_time_service} Sek.")
                time.sleep(sleep_time_service)
                continue
            except (ValueError, TypeError):
                if attempt >= INVALID_INPUT_MAX_ATTEMPTS:
                    log_error(f"Fehler bei den Eingabedaten nach {INVALID_INPUT_MAX_ATTEMPTS} Versuchen.")
                    break
                time.sleep(5)
                continue
            except Exception as ex:
                log_error("Unerwarteter Fehler.")
                return None

        log_error(f"Antwort vom Modell nach {attempts} Versuchen nicht erhalten.")
        return None

    def describe_image(
        self, image: Path | bytes, mime_type: Optional[str] = 'image/jpeg', prompt: Optional[str] = ''
    ) -> Optional[str]:
        """
        Die Funktion sendet ein Bild an das Gemini-Modell und gibt dessen Beschreibung zurÃ¼ck.

        Args:
            image (Path | bytes): Pfad zur Bilddatei oder Bytes.
            mime_type (Optional[str]): MIME-Typ des Bildes.
            prompt (Optional[str]): Text-Prompt fÃ¼r das Modell.

        Returns:
            Optional[str]: Textbeschreibung des Bildes oder `None` bei einem Fehler.
        """
        image_data: bytes
        if isinstance(image, Path):
            img_bytes = get_image_bytes(image)
            if img_bytes is None:
                return None
            image_data = img_bytes
        elif isinstance(image, bytes):
            image_data = image
        else:
            log_error(f"UngÃ¼ltiger Typ fÃ¼r 'image'. Erwartet wird Path oder bytes, erhalten: {type(image)}")
            return None

        content_parts: List[Any] = []
        if prompt:
            content_parts.append(prompt)
        content_parts.append(image_data)

        try:
            response = self.model.generate_content(content_parts)
            if hasattr(response, 'text') and response.text:
                return response.text
            else:
                log_info(f"Modell hat eine Antwort ohne Text zurÃ¼ckgegeben: {response}")
                return None
        except Exception as ex:
            log_error("Fehler bei der Bildverarbeitung.")
            return None
```


Diese Datei ist **bereit zur Verwendung** im `AIAssistant`-Addon fÃ¼r FreeCAD und integriert Ihren leistungsstarken Google Gemini-Konnektor vollstÃ¤ndig.

```mermaid
flowchart TD
    A[Initialisierung GoogleGenerativeAi] --> B{"Ist die API-Konfiguration erfolgreich?"}
    B -- "Nein" --> C["Ausnahme auslÃ¶sen: Authentifizierungs- oder Initialisierungsfehler"]
    B -- "Ja" --> D["Modell initialisiert. Chat-Sitzung _chat erstellt"]

    D --> E{"Welche Methode wurde aufgerufen?"}

    E -->|"ask stateless"| F["content_to_send mit RAG-Kontext erstellen"]
    F --> G["Versuchs-Schleife: bis attempts"]
    G --> H{"Antwort vom Modell erhalten?"}
    H -- "Nein" --> I{"Fehlertyp?"}

    I -->|"Netzwerk / ServiceUnavailable"| J["Pause + Wiederholung"]
    I -->|"ResourceExhausted"| K["RÃ¼ckgabe 'ResourceExhausted'"]
    I -->|"InvalidArgument / Auth"| L["RÃ¼ckgabe None"]
    H -- "Ja" --> M{"Antwort bereinigen?"}
    M -- "Ja" --> N["normalize_answer anwenden"]
    M -- "Nein" --> O["Rohen Text zurÃ¼ckgeben"]
    N --> P["Text zurÃ¼ckgeben"]
    O --> P

    E -->|"chat stateful"| Q["parts_to_send mit RAG-Kontext erstellen"]
    Q --> R["Senden Ã¼ber _chat.send_message_async"]
    R --> S{"Antwort erfolgreich?"}
    S -- "Nein" --> T{"Fehlertyp?"}
    T -->|"ResourceExhausted"| U["Lange Pause + Chat neu starten"]
    T -->|"Token-Ãœberschreitung"| V["Verlauf lÃ¶schen + Wiederholung"]
    T -->|"gRPC DEADLINE_EXCEEDED"| W["Pause + Chat neu starten"]
    S -- "Ja" --> X["Zu chat_history hinzufÃ¼gen: Benutzer â†’ Modell"]
    X --> Y["Asynchrones Speichern des Verlaufs in JSON"]
    Y --> Z["Text zurÃ¼ckgeben"]

    E -->|"describe_image"| AA["Bild lesen: Pfad â†’ Bytes"]
    AA --> AB{"Erfolgreich?"}
    AB -- "Nein" --> AC["RÃ¼ckgabe None"]
    AB -- "Ja" --> AD["Datei Ã¼ber genai.upload_file_async hochladen"]
    AD --> AE["Senden [prompt + file] an generate_content"]
    AE --> AF{"Antwort enthÃ¤lt Text?"}
    AF -- "Nein" --> AG["Feedback protokollieren, None zurÃ¼ckgeben"]
    AF -- "Ja" --> AH["Bildbeschreibung zurÃ¼ckgeben"]

    C --> AI["Abschluss"]
    P --> AI
    L --> AI
    K --> AI
    Z --> AI
    U --> AI
    V --> AI
    W --> AI
    AG --> AI
    AH --> AI

    ```